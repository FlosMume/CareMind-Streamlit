# %% [markdown]
# # ğŸ“„ Medical Guideline PDF Parser v1.1
# Enhanced to extract rich metadata from Chinese clinical guidelines and interpretation articles.
# Distinguishes between official guidelines and expert interpretations.
# Outputs: `guidelines.parsed.jsonl` with full bibliographic & structural metadata.

# %%
from pathlib import Path
import pdfplumber 
import re
import json
from typing import List, Dict, Any

# %%
# =============================
# ğŸ§© 1. METADATA EXTRACTION UTILS (from filename)
# =============================

def extract_year_from_filename(filename: str) -> str:
    """Extracts 4-digit year from Chinese medical guideline filenames."""
    patterns = [
        r'[ï¼ˆ\(]([12]\d{3})[ï¼‰\)]',
        r'[ï¼ˆ\(]([12]\d{3})[å¹´\s]*(?:ä¿®è®¢ç‰ˆ|ç‰ˆ|å¹´ç‰ˆ|å¹´)?[ï¼‰\)]?',
        r'([12]\d{3})[å¹´\s]*(?:ä¿®è®¢ç‰ˆ|ç‰ˆ|å¹´ç‰ˆ|å¹´)?(?=[\s_ï¼‰\)ã€‚\.\-]|$)',
        r'[ï¼ˆ\(]?([12]\d{3})[ï¼‰\)]?\s*\.pdf$',
    ]
    for pattern in patterns:
        match = re.search(pattern, filename)
        if match:
            return match.group(1)
    return "unknown"

def extract_doc_title(filename: str) -> str:
    """Extract clean document title from filename."""
    base = re.sub(r"_[^_]*\.pdf$", "", filename)
    base = re.sub(r"\.pdf$", "", base)
    base = re.sub(r"[ï¼ˆ\(][^ï¼‰\)]*[ï¼‰\)]", "", base)
    base = re.sub(r"\s*â€”+\s*", " ", base)
    base = re.sub(r"\s+", " ", base).strip()
    return base or "æœªå‘½åæ–‡æ¡£"

def extract_authors_from_filename(filename: str) -> List[str]:
    """Extract author names from filename (after last underscore)."""
    match = re.search(r"_([^_\(]+?)(?:\([12]\d{3}\))?\.pdf$", filename)
    if match:
        author_str = match.group(1).strip()
        authors = re.split(r"[,ï¼Œã€]", author_str)
        return [a.strip() for a in authors if a.strip()]
    return []

def extract_doc_type_from_filename(filename: str) -> str:
    """Classify document type from filename."""
    if "æŒ‡å—" in filename and "è§£è¯»" not in filename:
        return "guideline"
    elif "è§£è¯»" in filename or "æµ…æ" in filename or "è§£æ" in filename:
        return "guideline_interpretation"
    elif "å…±è¯†" in filename:
        return "consensus"
    elif "è¯æ®æ€»ç»“" in filename:
        return "evidence_summary"
    else:
        return "other"

# %%
# =============================
# ğŸ“š 2. METADATA EXTRACTION UTILS (from document text â€” PREFERRED)
# =============================

def extract_metadata_from_text(text: str) -> Dict[str, Any]:
    """Extract rich metadata from first page of document text."""
    meta = {
        "authors": [],
        "corresponding_author": "",
        "affiliations": [],
        "journal_name": "",
        "volume": "",
        "issue": "",
        "pages": "",
        "doi": "",
        "keywords": [],
        "publish_date": "",
        "original_guideline_title": "",
        "doc_type": "other"  # Will be overridden if detected
    }

    # Extract author (first non-empty line after title, before postal code or affiliation)
    lines = [ln.strip() for ln in text.splitlines() if ln.strip()] #
    for i, line in enumerate(lines[:10]):  # Look in first 10 lines
        if re.match(r"^\d{6}", line):  # Postal code â†’ previous line is likely author
            if i > 0:
                author_line = lines[i-1]
                authors = re.split(r"[,ï¼Œã€]", author_line)
                meta["authors"] = [a.strip() for a in authors if len(a.strip()) >= 2]
            break
        if "é€šä¿¡ä½œè€…" in line:
            author_match = re.search(r"é€šä¿¡ä½œè€…[:ï¼š]\s*([^\sï¼Œ,ã€]+)", line)
            if author_match:
                meta["corresponding_author"] = author_match.group(1).strip()
                if not meta["authors"]:
                    meta["authors"] = [meta["corresponding_author"]]

    # Extract affiliation (lines with postal code or university)
    for line in lines[:15]:
        if re.search(r"\d{6}|å¤§å­¦|åŒ»é™¢|ä¸­å¿ƒ", line) and len(line) > 10:
            meta["affiliations"].append(line)

    # Extract DOI
    doi_match = re.search(r"DOI\s*[:ï¼š]?\s*([0-9\.\s\/a-z-]+)", text, re.IGNORECASE)
    if doi_match:
        meta["doi"] = re.sub(r"\s+", "", doi_match.group(1)).strip()

    # Extract journal, volume, issue, pages from footer pattern
    # e.g., "Â·396Â· ä¸­å›½å¿ƒè¡€ç®¡æ‚å¿— 2024å¹´ 10æœˆç¬¬ 29å·ç¬¬ 5æœŸ"
    journal_match = re.search(r"Â·\d+Â·\s*([^\s]+?æ‚å¿—|å­¦æŠ¥)\s*(\d{4})å¹´\s*\d+æœˆç¬¬\s*(\d+)å·ç¬¬\s*(\d+)æœŸ", text)
    if journal_match:
        meta["journal_name"] = journal_match.group(1).strip()
        meta["publish_date"] = journal_match.group(2).strip()  # e.g., "2024"
        meta["volume"] = journal_match.group(3).strip()
        meta["issue"] = journal_match.group(4).strip()

    # Extract pages from header/footer (e.g., "Â·396Â·")
    page_match = re.search(r"Â·(\d+)Â·", text.splitlines()[0] if text.splitlines() else "")
    if page_match:
        start_page = page_match.group(1)
        # Try to find end page (often not available, so leave as single page)
        meta["pages"] = start_page

    # Extract keywords
    kw_match = re.search(r"ã€å…³é”®è¯ã€‘\s*([^\nã€ã€‘]+)", text)
    if kw_match:
        kw_str = kw_match.group(1).strip()
        meta["keywords"] = [k.strip() for k in re.split(r"[,ï¼Œ;ï¼›ã€]", kw_str) if k.strip()]

    # Detect if this is an interpretation of a guideline
    guideline_ref_match = re.search(r"ã€Š([^ã€‹]+?æŒ‡å—[^ã€‹]*)ã€‹", text[:500])
    if guideline_ref_match:
        meta["original_guideline_title"] = guideline_ref_match.group(1).strip()
        if "è§£è¯»" in text[:200] or "æµ…æ" in text[:200]:
            meta["doc_type"] = "guideline_interpretation"

    # If no authors found but filename has them, fallback
    # (Handled in main function)

    return meta

# %%
# =============================
# ğŸ“„ 3. TEXT EXTRACTION
# =============================

def extract_text_from_pdf(pdf_path: Path) -> str:
    """Extract text from PDF using pdfplumber."""
    try:
        with pdfplumber.open(str(pdf_path)) as pdf: 
            pages = [p.extract_text() or "" for p in pdf.pages] 
        return "\n".join(pages)
    except Exception as e:
        print(f"âš ï¸  PDF extraction error: {e}")
        return ""

# %%
# =============================
# ğŸ§± 4. CHUNKING LOGIC
# =============================

def chunk_by_rules(
    text: str,
    source_filename: str,
    year: str,
    doc_title: str,
    authors: List[str],
    doc_type: str,
    original_guideline_title: str = "",
    journal_name: str = "",
    volume: str = "",
    issue: str = "",
    pages: str = "",
    doi: str = "",
    keywords: List[str] = [],
    publish_date: str = ""
) -> List[Dict[str, Any]]:
    """Split text into chunks by section titles, with rich metadata."""
    lines = [ln.strip() for ln in text.splitlines() if ln.strip()]
    if not lines:
        return []

    chunks, buf = [], []
    current_title = "æœªå‘½åç« èŠ‚"

    TITLE_KEYWORDS = [
        "ç« ", "èŠ‚", "ç¯‡", "éƒ¨åˆ†", "æ¦‚è¿°", "èƒŒæ™¯", "ç›®çš„", "æ–¹æ³•", "ç»“æœ", "ç»“è®º",
        "æ¨è", "å»ºè®®", "ç®¡ç†", "æ²»ç–—", "è¯Šæ–­", "è¯„ä¼°", "å®šä¹‰", "ç›®æ ‡",
        "ä¸€ã€", "äºŒã€", "ä¸‰ã€", "å››ã€", "äº”ã€", "å…­ã€", "ä¸ƒã€", "å…«ã€", "ä¹ã€", "åã€",
        "1.", "2.", "3.", "4.", "5.", "6.", "7.", "8.", "9.", "10.",
        "ç¬¬ä¸€", "ç¬¬äºŒ", "ç¬¬ä¸‰", "ç¬¬å››", "ç¬¬äº”",
        "ã€", "ã€‘", "ï¼ˆ", "ï¼‰", "(", ")", "ï¼š", ":"
    ]

    for ln in lines:
        is_title = False

        if any(kw in ln for kw in TITLE_KEYWORDS) and 3 <= len(ln) <= 100:
            is_title = True
        elif ln.endswith("ï¼š") or ln.endswith(":") or \
             (len(ln) <= 50 and (ln.startswith("ã€") and ln.endswith("ã€‘"))):
            is_title = True
        elif 3 <= len(ln) <= 25 and not ln.endswith("ã€‚") and not ln.endswith("."):
            is_title = True
        elif re.match(r"^[0-9]+[\.ã€]\s*\S{3,}", ln):
            is_title = True

        if is_title:
            if buf:
                chunk_id = f"{re.sub(r'[^a-zA-Z0-9]', '_', doc_title)}_{year}_{len(chunks):03d}"
                chunk_meta = {
                    "source_filename": source_filename,
                    "doc_title": doc_title,
                    "section_title": current_title,
                    "authors": authors,
                    "year": year,
                    "doc_type": doc_type,
                    "original_guideline_title": original_guideline_title,
                    "journal_name": journal_name,
                    "volume": volume,
                    "issue": issue,
                    "pages": pages,
                    "doi": doi,
                    "keywords": keywords,
                    "publish_date": publish_date,
                    "chunk_id": chunk_id,
                    "extraction_method": "pdfplumber + rule-based + metadata extraction"
                }
                chunks.append({
                    "content": "\n".join(buf),
                    "meta": chunk_meta
                })
                buf = []
            current_title = ln
        else:
            buf.append(ln)

    # Flush final buffer
    if buf:
        chunk_id = f"{re.sub(r'[^a-zA-Z0-9]', '_', doc_title)}_{year}_{len(chunks):03d}"
        chunk_meta = {
            "source_filename": source_filename,
            "doc_title": doc_title,
            "section_title": current_title,
            "authors": authors,
            "year": year,
            "doc_type": doc_type,
            "original_guideline_title": original_guideline_title,
            "journal_name": journal_name,
            "volume": volume,
            "issue": issue,
            "pages": pages,
            "doi": doi,
            "keywords": keywords,
            "publish_date": publish_date,
            "chunk_id": chunk_id,
            "extraction_method": "pdfplumber + rule-based + metadata extraction"
        }
        chunks.append({
            "content": "\n".join(buf),
            "meta": chunk_meta
        })

    return chunks

# %%
# =============================
# ğŸš€ 5. MAIN PROCESSING PIPELINE
# =============================

def main():
    in_dir = Path("data/guidelines")
    out_path = Path("data/guidelines.parsed.jsonl")

    print(f"ğŸ“‚ Input directory: {in_dir.absolute()}")
    pdf_files = list(in_dir.glob("*.pdf"))
    print(f"ğŸ“„ Found {len(pdf_files)} PDF files.")

    if not pdf_files:
        print("âŒ No PDFs found. Check directory path.")
        return

    with out_path.open("w", encoding="utf-8") as f:
        for pdf in pdf_files:
            print(f"\n--- ğŸ“„ Processing: {pdf.name} ---")

            # Step 1: Extract preliminary metadata from filename
            year = extract_year_from_filename(pdf.name)
            doc_title = extract_doc_title(pdf.name)
            authors_filename = extract_authors_from_filename(pdf.name)
            doc_type = extract_doc_type_from_filename(pdf.name)

            print(f"  ğŸ“… Year (from filename): {year}")
            print(f"  ğŸ·ï¸ Title (from filename): {doc_title}")
            print(f"  ğŸ‘¥ Authors (from filename): {authors_filename}")
            print(f"  ğŸ“‘ Type (from filename): {doc_type}")

            # Step 2: Extract text
            text = extract_text_from_pdf(pdf)
            char_count = len(text.strip())
            print(f"  ğŸ”¤ Extracted {char_count} characters.")

            if char_count == 0:
                print("  âš ï¸  WARNING: No text extracted. File may be scanned.")
                continue

            # Step 3: Extract rich metadata from text (overrides filename where possible)
            text_meta = extract_metadata_from_text(text)

            # Merge: Prefer text-extracted metadata, fallback to filename
            authors = text_meta["authors"] if text_meta["authors"] else authors_filename
            doc_type_final = text_meta["doc_type"] if text_meta["doc_type"] != "other" else doc_type
            original_guideline_title = text_meta["original_guideline_title"]
            journal_name = text_meta["journal_name"]
            volume = text_meta["volume"]
            issue = text_meta["issue"]
            pages = text_meta["pages"]
            doi = text_meta["doi"]
            keywords = text_meta["keywords"]
            publish_date = text_meta["publish_date"]

            print(f"  âœï¸  Authors (final): {authors}")
            print(f"  ğŸ“š Journal: {journal_name} {volume}({issue}), {pages}, {publish_date}")
            print(f"  ğŸ”— DOI: {doi}")
            print(f"  ğŸ·ï¸ Keywords: {keywords}")
            print(f"  ğŸ§­ Original Guideline: {original_guideline_title}")
            print(f"  ğŸ“‘ Doc Type (final): {doc_type_final}")

            # Step 4: Chunk text with full metadata
            chunks = chunk_by_rules(
                text=text,
                source_filename=pdf.name,
                year=year,
                doc_title=doc_title,
                authors=authors,
                doc_type=doc_type_final,
                original_guideline_title=original_guideline_title,
                journal_name=journal_name,
                volume=volume,
                issue=issue,
                pages=pages,
                doi=doi,
                keywords=keywords,
                publish_date=publish_date
            )
            print(f"  ğŸ§© Generated {len(chunks)} chunks.")

            if not chunks:
                print("  âŒ No chunks generated. Check chunking logic.")

            # Step 5: Write to JSONL
            for chunk in chunks:
                f.write(json.dumps(chunk, ensure_ascii=False) + "\n")

    print(f"\nâœ… Output written to: {out_path.absolute()}")
    print(f"ğŸ’¾ File size: {out_path.stat().st_size} bytes")

# %%
# =============================
# â–¶ï¸ 6. RUN
# =============================

if __name__ == "__main__":
    main()